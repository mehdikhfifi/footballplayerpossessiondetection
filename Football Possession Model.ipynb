{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setting up\n"
      ],
      "metadata": {
        "id": "auoBFjsg7713"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuoNpW-q8E1P",
        "outputId": "6009a93c-7a00-46c5-9e0d-3065e84c83a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 27 20:22:42 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0              24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpTF5Arf70zV",
        "outputId": "06ad68c1-5126-491c-e3b3-8540f9fe8895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Data Set I will be using will be imported from the Bundesliga Challenge in Kaggle, however, you can use any video you wish"
      ],
      "metadata": {
        "id": "lsQ45uiL8P6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "P_A7FLvw8Khu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = getpass('Enter KAGGLE_USERNAME secret value: ')\n",
        "os.environ['KAGGLE_KEY'] = getpass('Enter KAGGLE_KEY secret value: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_5CPUYE8DVN",
        "outputId": "93c23145-1d09-4b8b-bac9-abdc9067c798"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter KAGGLE_USERNAME secret value: ··········\n",
            "Enter KAGGLE_KEY secret value: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and unzipping DataSet"
      ],
      "metadata": {
        "id": "m7n6Z8u494DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions files -c dfl-bundesliga-data-shootout | grep clips | head -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orAPuTqe9-S3",
        "outputId": "049beb9e-70d7-4c86-b685-73908f464cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clips/08fd33_2.mp4      21MB  2022-07-29 14:23:09  \n",
            "clips/08fd33_9.mp4      18MB  2022-07-29 14:23:09  \n",
            "clips/0a2d9b_9.mp4      18MB  2022-07-29 14:23:09  \n",
            "clips/08fd33_3.mp4      17MB  2022-07-29 14:23:09  \n",
            "clips/08fd33_6.mp4      19MB  2022-07-29 14:23:09  \n",
            "clips/08fd33_0.mp4      19MB  2022-07-29 14:23:09  \n",
            "clips/0a2d9b_5.mp4      18MB  2022-07-29 14:23:09  \n",
            "clips/08fd33_7.mp4      18MB  2022-07-29 14:23:09  \n",
            "clips/0a2d9b_0.mp4      20MB  2022-07-29 14:23:09  \n",
            "clips/0a2d9b_7.mp4      19MB  2022-07-29 14:23:09  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!kaggle competitions files -c dfl-bundesliga-data-shootout | \\\n",
        "grep clips | head -20 | \\\n",
        "awk '{print $1}' | \\\n",
        "while read -r line; \\\n",
        "  do kaggle competitions download -c dfl-bundesliga-data-shootout -f $line -p clips --quiet; \\\n",
        "  unzip ${line}.zip -d clips && rm ${line}.zip; \\\n",
        "  done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRgN4xTE8zAF",
        "outputId": "d3d1a5e5-45b1-412c-cf35-45a74deedcb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '{HOME}'\n",
            "/content\n",
            "Archive:  clips/08fd33_2.mp4.zip\n",
            "  inflating: clips/08fd33_2.mp4      \n",
            "Archive:  clips/08fd33_9.mp4.zip\n",
            "  inflating: clips/08fd33_9.mp4      \n",
            "Archive:  clips/0a2d9b_9.mp4.zip\n",
            "  inflating: clips/0a2d9b_9.mp4      \n",
            "Archive:  clips/08fd33_3.mp4.zip\n",
            "  inflating: clips/08fd33_3.mp4      \n",
            "Archive:  clips/08fd33_6.mp4.zip\n",
            "  inflating: clips/08fd33_6.mp4      \n",
            "Archive:  clips/08fd33_0.mp4.zip\n",
            "  inflating: clips/08fd33_0.mp4      \n",
            "Archive:  clips/0a2d9b_5.mp4.zip\n",
            "  inflating: clips/0a2d9b_5.mp4      \n",
            "Archive:  clips/08fd33_7.mp4.zip\n",
            "  inflating: clips/08fd33_7.mp4      \n",
            "Archive:  clips/0a2d9b_0.mp4.zip\n",
            "  inflating: clips/0a2d9b_0.mp4      \n",
            "Archive:  clips/0a2d9b_7.mp4.zip\n",
            "  inflating: clips/0a2d9b_7.mp4      \n",
            "Archive:  clips/08fd33_1.mp4.zip\n",
            "  inflating: clips/08fd33_1.mp4      \n",
            "Archive:  clips/0a2d9b_2.mp4.zip\n",
            "  inflating: clips/0a2d9b_2.mp4      \n",
            "Archive:  clips/0a2d9b_1.mp4.zip\n",
            "  inflating: clips/0a2d9b_1.mp4      \n",
            "Archive:  clips/08fd33_4.mp4.zip\n",
            "  inflating: clips/08fd33_4.mp4      \n",
            "Archive:  clips/0a2d9b_4.mp4.zip\n",
            "  inflating: clips/0a2d9b_4.mp4      \n",
            "Archive:  clips/0a2d9b_3.mp4.zip\n",
            "  inflating: clips/0a2d9b_3.mp4      \n",
            "Archive:  clips/08fd33_5.mp4.zip\n",
            "  inflating: clips/08fd33_5.mp4      \n",
            "Archive:  clips/0a2d9b_6.mp4.zip\n",
            "  inflating: clips/0a2d9b_6.mp4      \n",
            "Archive:  clips/0a2d9b_8.mp4.zip\n",
            "  inflating: clips/0a2d9b_8.mp4      \n",
            "Archive:  clips/08fd33_8.mp4.zip\n",
            "  inflating: clips/08fd33_8.mp4      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Yolo V5"
      ],
      "metadata": {
        "id": "1AnJ9CKY-PmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lbS81K4N-E44",
        "outputId": "b136cadf-791d-4bab-d4f5-022e7d7822bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-254-gba63208 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla V100-SXM2-16GB, 16151MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 26.7/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model using the [football-players-detection](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fapp.roboflow.com%2Froboflow-jvuqo%2Ffootball-players-detection-3zvbc%2Foverview) dataset found on roboflow"
      ],
      "metadata": {
        "id": "iAoyxZoABJ8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1OYwrlRti4cieuvVr8ERaJhTQdFJXWT4I' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1OYwrlRti4cieuvVr8ERaJhTQdFJXWT4I\" -O best.pt && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "iMhIUSvC-R4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_PATH = f\"{HOME}/best.pt\""
      ],
      "metadata": {
        "id": "XuENvllfBXHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}/yolov5\n",
        "!python detect.py --weights {HOME}/best.pt --img 1280 --conf 0.25 --source {HOME}/clips/08fd33_4.mp4 --name custom"
      ],
      "metadata": {
        "id": "PoTS6fbBBYv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going forward, we need to test our model on a single frame"
      ],
      "metadata": {
        "id": "ftiVYtgkBcDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Generator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def generate_frames(video_file: str) -> Generator[np.ndarray, None, None]:\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        yield frame\n",
        "\n",
        "    video.release()\n",
        "\n",
        "\n",
        "def plot_image(image: np.ndarray, size: int = 12) -> None:\n",
        "    plt.figure(figsize=(size, size))\n",
        "    plt.imshow(image[...,::-1])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SUeO7XVzBamD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_VIDEO_PATH = f\"{HOME}/clips/08fd33_4.mp4\""
      ],
      "metadata": {
        "id": "Zz-XJybLBhwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))"
      ],
      "metadata": {
        "id": "QGqCVcL6Bi0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame = next(frame_iterator)\n",
        "plot_image(frame, 16)"
      ],
      "metadata": {
        "id": "EdtuxM7wBjoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import ultralytics as well to help us in the object detection"
      ],
      "metadata": {
        "id": "XuFAMpqyBmIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', WEIGHTS_PATH, device=0)"
      ],
      "metadata": {
        "id": "HSmi4H69BlSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model(frame, size=1280)"
      ],
      "metadata": {
        "id": "yEAuXdB1BvfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.pandas()"
      ],
      "metadata": {
        "id": "3EhbmKDmBwqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.pred[0]"
      ],
      "metadata": {
        "id": "9D4LU2L_Bxoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For object tracking, we cannot merely rely on Yolo. We need to install a tracking model, ByteTrack, as well as other libraries to support us in this goal.\n",
        "\n",
        "The installation of ByteTrack is not straightforward, it requires more package manipulation and organization."
      ],
      "metadata": {
        "id": "Xdyc7f9OBzMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "!cd ByteTrack && pip3 install -r requirements.txt\n",
        "!cd ByteTrack && python3 setup.py develop\n",
        "!pip install cython_bbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vri-xCYSBylc",
        "outputId": "fb065642-a33f-49a7-ba28-f1d94f26dccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '{HOME}'\n",
            "/content/yolov5\n",
            "Cloning into 'ByteTrack'...\n",
            "remote: Enumerating objects: 2007, done.\u001b[K\n",
            "remote: Total 2007 (delta 0), reused 0 (delta 0), pack-reused 2007\u001b[K\n",
            "Receiving objects: 100% (2007/2007), 79.60 MiB | 26.07 MiB/s, done.\n",
            "Resolving deltas: 100% (1141/1141), done.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.1.0+cu121)\n",
            "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.8.0.76)\n",
            "Collecting loguru (from -r requirements.txt (line 5))\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.66.1)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.16.0+cu121)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (10.1.0)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.1.1.post2209072238)\n",
            "Collecting ninja (from -r requirements.txt (line 11))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2.15.1)\n",
            "Collecting lap (from -r requirements.txt (line 14))\n",
            "  Downloading lap-0.4.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting motmetrics (from -r requirements.txt (line 15))\n",
            "  Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filterpy (from -r requirements.txt (line 16))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.9.0)\n",
            "Collecting onnx==1.8.1 (from -r requirements.txt (line 20))\n",
            "  Downloading onnx-1.8.1.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime==1.8.0 (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2, 1.16.3)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime==1.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0mrunning develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating yolox.egg-info\n",
            "writing yolox.egg-info/PKG-INFO\n",
            "writing dependency_links to yolox.egg-info/dependency_links.txt\n",
            "writing top-level names to yolox.egg-info/top_level.txt\n",
            "writing manifest file 'yolox.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'yolox.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'yolox.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "building 'yolox._C' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/content\n",
            "creating build/temp.linux-x86_64-cpython-310/content/yolov5\n",
            "creating build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack\n",
            "creating build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox\n",
            "creating build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox/layers\n",
            "creating build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox/layers/csrc\n",
            "creating build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox/layers/csrc/cocoeval\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/yolov5/ByteTrack/yolox/layers/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c /content/yolov5/ByteTrack/yolox/layers/csrc/cocoeval/cocoeval.cpp -o build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox/layers/csrc/cocoeval/cocoeval.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/content/yolov5/ByteTrack/yolox/layers/csrc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c /content/yolov5/ByteTrack/yolox/layers/csrc/vision.cpp -o build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox/layers/csrc/vision.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/yolox\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox/layers/csrc/cocoeval/cocoeval.o build/temp.linux-x86_64-cpython-310/content/yolov5/ByteTrack/yolox/layers/csrc/vision.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/yolox/_C.cpython-310-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-cpython-310/yolox/_C.cpython-310-x86_64-linux-gnu.so -> yolox\n",
            "Creating /usr/local/lib/python3.10/dist-packages/yolox.egg-link (link to .)\n",
            "Adding yolox 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/yolov5/ByteTrack\n",
            "Processing dependencies for yolox==0.1.0\n",
            "Finished processing dependencies for yolox==0.1.0\n",
            "Collecting cython_bbox\n",
            "  Downloading cython_bbox-0.1.5.tar.gz (4.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from cython_bbox) (3.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cython_bbox) (1.23.5)\n",
            "Building wheels for collected packages: cython_bbox\n",
            "  Building wheel for cython_bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cython_bbox: filename=cython_bbox-0.1.5-cp310-cp310-linux_x86_64.whl size=99056 sha256=97a70c38c66025ccb38e46041455c5b12a107dbacb950b8bfd97fa34c1589489\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/b7/68/bab98b7180cda501101a57fb7d36884218ad45ec60c27cd679\n",
            "Successfully built cython_bbox\n",
            "Installing collected packages: cython_bbox\n",
            "Successfully installed cython_bbox-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(f\"{HOME}/ByteTrack\")"
      ],
      "metadata": {
        "id": "SAnCoyH_CFl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onemetric --quiet"
      ],
      "metadata": {
        "id": "1Z4aVWFQCIxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    track_thresh: float = 0.25\n",
        "    track_buffer: int = 30\n",
        "    match_thresh: float = 0.8\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "    min_box_area: float = 1.0\n",
        "    mot20: bool = False"
      ],
      "metadata": {
        "id": "tZBSCtXICJ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, Loguru may already be installed your machine, in which case you can remove the pip install"
      ],
      "metadata": {
        "id": "fjN5Twz5CNnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru\n",
        "!pip install lap\n",
        "\n",
        "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "from onemetric.cv.utils.iou import box_iou_batch"
      ],
      "metadata": {
        "id": "evlyXmVKCM5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I build the classes needed for annotation as well as for player side determination. The most important class here is BaseAnnotator and Detection, as they are the keystones in determining the color of the bounding box on players."
      ],
      "metadata": {
        "id": "V8V-t2vQCUDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Tuple, Optional, List, Dict, Any\n",
        "\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# geometry utilities\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Point:\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "    @property\n",
        "    def int_xy_tuple(self) -> Tuple[int, int]:\n",
        "        return int(self.x), int(self.y)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Rect:\n",
        "    x: float\n",
        "    y: float\n",
        "    width: float\n",
        "    height: float\n",
        "\n",
        "    @property\n",
        "    def min_x(self) -> float:\n",
        "        return self.x\n",
        "\n",
        "    @property\n",
        "    def min_y(self) -> float:\n",
        "        return self.y\n",
        "\n",
        "    @property\n",
        "    def max_x(self) -> float:\n",
        "        return self.x + self.width\n",
        "\n",
        "    @property\n",
        "    def max_y(self) -> float:\n",
        "        return self.y + self.height\n",
        "\n",
        "    @property\n",
        "    def top_left(self) -> Point:\n",
        "        return Point(x=self.x, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def bottom_right(self) -> Point:\n",
        "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def bottom_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height)\n",
        "\n",
        "    @property\n",
        "    def top_center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y)\n",
        "\n",
        "    @property\n",
        "    def center(self) -> Point:\n",
        "        return Point(x=self.x + self.width / 2, y=self.y + self.height / 2)\n",
        "\n",
        "    def pad(self, padding: float) -> Rect:\n",
        "        return Rect(\n",
        "            x=self.x - padding,\n",
        "            y=self.y - padding,\n",
        "            width=self.width + 2*padding,\n",
        "            height=self.height + 2*padding\n",
        "        )\n",
        "\n",
        "    def contains_point(self, point: Point) -> bool:\n",
        "        return self.min_x < point.x < self.max_x and self.min_y < point.y < self.max_y\n",
        "\n",
        "\n",
        "# detection utilities\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Detection:\n",
        "    rect: Rect\n",
        "    class_id: int\n",
        "    class_name: str\n",
        "    confidence: float\n",
        "    player_attribute: Optional[int] = 0\n",
        "    tracker_id: Optional[int] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_results(cls, pred: np.ndarray, names: Dict[int, str]) -> List[Detection]:\n",
        "        result = []\n",
        "        for x_min, y_min, x_max, y_max, confidence, class_id in pred:\n",
        "            class_id=int(class_id)\n",
        "            result.append(Detection(\n",
        "                rect=Rect(\n",
        "                    x=float(x_min),\n",
        "                    y=float(y_min),\n",
        "                    width=float(x_max - x_min),\n",
        "                    height=float(y_max - y_min)\n",
        "                ),\n",
        "                class_id=class_id,\n",
        "                class_name=names[class_id],\n",
        "                confidence=float(confidence),\n",
        "            ))\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def filter_detections_by_class(detections: List[Detection], class_name: str) -> List[Detection]:\n",
        "    return [detection for detection in detections if detection.class_name == class_name]\n",
        "\n",
        "\n",
        "\n",
        "# draw utilities\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Color:\n",
        "    r: int\n",
        "    g: int\n",
        "    b: int\n",
        "\n",
        "    @property\n",
        "    def bgr_tuple(self) -> Tuple[int, int, int]:\n",
        "        return self.b, self.g, self.r\n",
        "\n",
        "    @classmethod\n",
        "    def from_hex_string(cls, hex_string: str) -> Color:\n",
        "        r, g, b = tuple(int(hex_string[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
        "        return Color(r=r, g=g, b=b)\n",
        "\n",
        "\n",
        "def draw_rect(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, thickness)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_filled_rect(image: np.ndarray, rect: Rect, color: Color) -> np.ndarray:\n",
        "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, -1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_polygon(image: np.ndarray, countour: np.ndarray, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.drawContours(image, [countour], 0, color.bgr_tuple, thickness)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_filled_polygon(image: np.ndarray, countour: np.ndarray, color: Color) -> np.ndarray:\n",
        "    cv2.drawContours(image, [countour], 0, color.bgr_tuple, -1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_text(image: np.ndarray, anchor: Point, text: str, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.putText(image, text, anchor.int_xy_tuple, cv2.FONT_HERSHEY_SIMPLEX, 0.7, color.bgr_tuple, thickness, 2, False)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_ellipse(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
        "    cv2.ellipse(\n",
        "        image,\n",
        "        center=rect.bottom_center.int_xy_tuple,\n",
        "        axes=(int(rect.width), int(0.35 * rect.width)),\n",
        "        angle=0.0,\n",
        "        startAngle=-45,\n",
        "        endAngle=235,\n",
        "        color=color.bgr_tuple,\n",
        "        thickness=thickness,\n",
        "        lineType=cv2.LINE_4\n",
        "    )\n",
        "    return image\n",
        "\n",
        "\n",
        "# base annotator\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BaseAnnotator:\n",
        "    colors: List[Color]\n",
        "    thickness: int\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            annotated_image = draw_ellipse(\n",
        "                image=image,\n",
        "                rect=detection.rect,\n",
        "                color=  self.colors[detection.player_attribute] if detection.player_attribute else self.colors[detection.class_id],\n",
        "                thickness=self.thickness\n",
        "            )\n",
        "        return annotated_image"
      ],
      "metadata": {
        "id": "IWk_f17rCRwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Average Color is the class that will determine whether a player is on the left or right side team, using grayscale to split the classifications."
      ],
      "metadata": {
        "id": "TSUP30gJCx4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_color(image, bbox, threshold_value):\n",
        "    \"\"\"\n",
        "    Calculate the average color of the area defined by bbox in the image.\n",
        "    bbox is expected to be a tuple (x1, y1, x2, y2).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    x1 = bbox.x\n",
        "    # print(x1)\n",
        "    y1 =  bbox.y\n",
        "    # print(y1)\n",
        "    wdith =  bbox.width\n",
        "    # print(wdith)\n",
        "    height =  bbox.height\n",
        "    # print(height)\n",
        "\n",
        "    x2 = x1 + wdith\n",
        "    y2 = y1 + height\n",
        "    cropped_area = image[round(y1):round(y2), round(x1):round(x2)]\n",
        "    gray_image = cv2.cvtColor(cropped_area, cv2.COLOR_BGR2GRAY)\n",
        "    average_color = np.mean(cropped_area, axis=(0, 1))\n",
        "    average_color = 0.2126 * average_color[0] + 0.7152 * average_color[1] + 0.0722 * average_color[2]\n",
        "    # print(average_color)\n",
        "    # print(threshold_value)\n",
        "    if average_color > threshold_value:\n",
        "      color_label = 4\n",
        "    else:\n",
        "      color_label = 5\n",
        "\n",
        "    return color_label\n"
      ],
      "metadata": {
        "id": "yMcBC8BQCovw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provided below is the code needed to determine Ball Possession"
      ],
      "metadata": {
        "id": "ANa_2bCYDEz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# black\n",
        "MARKER_CONTOUR_COLOR_HEX = \"000000\"\n",
        "MARKER_CONTOUR_COLOR = Color.from_hex_string(MARKER_CONTOUR_COLOR_HEX)\n",
        "\n",
        "# red\n",
        "PLAYER_MARKER_FILL_COLOR_HEX = \"FF0000\"\n",
        "PLAYER_MARKER_FILL_COLOR = Color.from_hex_string(PLAYER_MARKER_FILL_COLOR_HEX)\n",
        "\n",
        "# green\n",
        "BALL_MARKER_FILL_COLOR_HEX = \"00FF00\"\n",
        "BALL_MARKER_FILL_COLOR = Color.from_hex_string(BALL_MARKER_FILL_COLOR_HEX)\n",
        "\n",
        "MARKER_CONTOUR_THICKNESS = 2\n",
        "MARKER_WIDTH = 20\n",
        "MARKER_HEIGHT = 20\n",
        "MARKER_MARGIN = 10\n",
        "\n",
        "# distance in pixels from the player's bounding box where we consider the ball is in his possession\n",
        "PLAYER_IN_POSSESSION_PROXIMITY = 30"
      ],
      "metadata": {
        "id": "fGVsi_W-DHHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# calculates coordinates of possession marker\n",
        "def calculate_marker(anchor: Point) -> np.ndarray:\n",
        "    x, y = anchor.int_xy_tuple\n",
        "    return(np.array([\n",
        "        [x - MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN],\n",
        "        [x, y - MARKER_MARGIN],\n",
        "        [x + MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN]\n",
        "    ]))\n",
        "\n",
        "#DRAWING\n",
        "# draw single possession marker\n",
        "def draw_marker(image: np.ndarray, anchor: Point, color: Color) -> np.ndarray:\n",
        "    possession_marker_countour = calculate_marker(anchor=anchor)\n",
        "    image = draw_filled_polygon(\n",
        "        image=image,\n",
        "        countour=possession_marker_countour,\n",
        "        color=color)\n",
        "    image = draw_polygon(\n",
        "        image=image,\n",
        "        countour=possession_marker_countour,\n",
        "        color=MARKER_CONTOUR_COLOR,\n",
        "        thickness=MARKER_CONTOUR_THICKNESS)\n",
        "    return image\n",
        "\n",
        "#DRAWING\n",
        "# dedicated annotator to draw possession markers on video frames\n",
        "@dataclass\n",
        "class MarkerAnntator:\n",
        "\n",
        "    color: Color\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            annotated_image = draw_marker(\n",
        "                image=image,\n",
        "                anchor=detection.rect.top_center,\n",
        "                color=self.color)\n",
        "        return annotated_image"
      ],
      "metadata": {
        "id": "Ni5psooADJCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "globalplayerdetection = ['Nothing', 'Nothing']\n",
        "# resolves which player is currently in ball possession based on player-ball proximity\n",
        "def get_player_in_possession(\n",
        "    player_detections: List[Detection],\n",
        "    ball_detections: List[Detection],\n",
        "    proximity: int\n",
        ") -> Optional[Detection]:\n",
        "    if len(ball_detections) != 1:\n",
        "        return None\n",
        "    ball_detection = ball_detections[0]\n",
        "    globalpasscounter = 0\n",
        "    for player_detection in player_detections:\n",
        "        if player_detection.rect.pad(proximity).contains_point(point=ball_detection.rect.center):\n",
        "            if player_detection != globalplayerdetection[-1]:\n",
        "                globalpasscounter +=1\n",
        "                globalplayerdetection.append(player_detection)\n",
        "            return player_detection"
      ],
      "metadata": {
        "id": "YNEIPO_jDKYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing out utilities"
      ],
      "metadata": {
        "id": "FiA50mwzDRLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate annotators\n",
        "ball_marker_annotator = MarkerAnntator(color=BALL_MARKER_FILL_COLOR)\n",
        "player_marker_annotator = MarkerAnntator(color=PLAYER_MARKER_FILL_COLOR)\n",
        "\n",
        "# acquire video frame\n",
        "frame = next(frame_iterator)\n",
        "\n",
        "# run detector\n",
        "results = model(frame, size=1280)\n",
        "detections = Detection.from_results(\n",
        "    pred=results.pred[0].cpu().numpy(),\n",
        "    names=model.names)\n",
        "\n",
        "# postprocess results\n",
        "ball_detections = filter_detections_by_class(detections=detections, class_name=\"ball\")\n",
        "player_detections = filter_detections_by_class(detections=detections, class_name=\"player\")\n",
        "player_in_possession_detection = get_player_in_possession(\n",
        "    player_detections=player_detections,\n",
        "    ball_detections=ball_detections,\n",
        "    proximity=PLAYER_IN_POSSESSION_PROXIMITY)\n",
        "\n",
        "# annotate video frame\n",
        "annotated_image = frame.copy()\n",
        "annotated_image = ball_marker_annotator.annotate(\n",
        "    image=annotated_image,\n",
        "    detections=ball_detections)\n",
        "annotated_image = player_marker_annotator.annotate(\n",
        "    image=annotated_image,\n",
        "    detections=[player_in_possession_detection] if player_in_possession_detection else [])\n",
        "\n",
        "# plot video frame\n",
        "plot_image(annotated_image, 16)"
      ],
      "metadata": {
        "id": "CbhXvfOeDMS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More utilities and tests for full video tracking"
      ],
      "metadata": {
        "id": "eS8n9s_uDPIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class VideoConfig:\n",
        "    fps: float\n",
        "    width: int\n",
        "    height: int\n",
        "\n",
        "\n",
        "# create cv2.VideoWriter object that we can use to save output video\n",
        "def get_video_writer(target_video_path: str, video_config: VideoConfig) -> cv2.VideoWriter:\n",
        "    video_target_dir = os.path.dirname(os.path.abspath(target_video_path))\n",
        "    os.makedirs(video_target_dir, exist_ok=True)\n",
        "    return cv2.VideoWriter(\n",
        "        target_video_path,\n",
        "        fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps=video_config.fps,\n",
        "        frameSize=(video_config.width, video_config.height),\n",
        "        isColor=True"
      ],
      "metadata": {
        "id": "sqaQVfPPDVhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "BYTETracker does not assign tracker_id to existing bounding boxes but rather\n",
        "predicts the next bounding box position based on previous one. Therefore, we\n",
        "need to find a way to match our bounding boxes with predictions.\n",
        "\n",
        "usage example:\n",
        "\n",
        "byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "for frame in frames:\n",
        "    ...\n",
        "    results = model(frame, size=1280)\n",
        "    detections = Detection.from_results(\n",
        "        pred=results.pred[0].cpu().numpy(),\n",
        "        names=model.names)\n",
        "    ...\n",
        "    tracks = byte_tracker.update(\n",
        "        output_results=detections2boxes(detections=detections),\n",
        "        img_info=frame.shape,\n",
        "        img_size=frame.shape)\n",
        "    detections = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
        "\"\"\"\n",
        "\n",
        "# converts List[Detection] into format that can be consumed by match_detections_with_tracks function\n",
        "def detections2boxes(detections: List[Detection], with_confidence: bool = True) -> np.ndarray:\n",
        "    return np.array([\n",
        "        [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y,\n",
        "            detection.confidence\n",
        "        ] if with_confidence else [\n",
        "            detection.rect.top_left.x,\n",
        "            detection.rect.top_left.y,\n",
        "            detection.rect.bottom_right.x,\n",
        "            detection.rect.bottom_right.y\n",
        "        ]\n",
        "        for detection\n",
        "        in detections\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
        "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
        "    return np.array([\n",
        "        track.tlbr\n",
        "        for track\n",
        "        in tracks\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# matches our bounding boxes with predictions\n",
        "def match_detections_with_tracks(\n",
        "    detections: List[Detection],\n",
        "    tracks: List[STrack]\n",
        ") -> List[Detection]:\n",
        "    detection_boxes = detections2boxes(detections=detections, with_confidence=False)\n",
        "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
        "    iou = box_iou_batch(tracks_boxes, detection_boxes)\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        if iou[tracker_index, detection_index] != 0:\n",
        "            detections[detection_index].tracker_id = tracks[tracker_index].track_id\n",
        "    return detections"
      ],
      "metadata": {
        "id": "OeQ0yHiRDZfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text annotator to display tracker_id\n",
        "@dataclass\n",
        "class TextAnnotator:\n",
        "    background_color: Color\n",
        "    text_color: Color\n",
        "    text_thickness: int\n",
        "\n",
        "    def annotate(self, image: np.ndarray, detections: List[Detection]) -> np.ndarray:\n",
        "        annotated_image = image.copy()\n",
        "        for detection in detections:\n",
        "            # if tracker_id is not assigned skip annotation\n",
        "\n",
        "            if detection.class_id == 2 or detection.class_id == 3:\n",
        "                detection.class_id = get_average_color(image, detection.rect, 150)\n",
        "            # print(detection.class_id)\n",
        "            display_text = f\"ID: {detection.tracker_id}, Color: {detection.player_attribute}\"\n",
        "\n",
        "            # calculate text dimensions\n",
        "            size, _ = cv2.getTextSize(\n",
        "                str(detection.tracker_id),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.7,\n",
        "                thickness=self.text_thickness)\n",
        "            width, height = size\n",
        "\n",
        "            # calculate text background position\n",
        "            center_x, center_y = detection.rect.bottom_center.int_xy_tuple\n",
        "            x = center_x - width // 2\n",
        "            y = center_y - height // 2 + 10\n",
        "\n",
        "            # draw background\n",
        "            annotated_image = draw_filled_rect(\n",
        "                image=annotated_image,\n",
        "                rect=Rect(x=x, y=y, width=width, height=height).pad(padding=5),\n",
        "                color=self.background_color)\n",
        "\n",
        "            # draw text\n",
        "            annotated_image = draw_text(\n",
        "                image=annotated_image,\n",
        "                anchor=Point(x=x, y=y + height),\n",
        "                text=str(detection.tracker_id),\n",
        "                color=self.text_color,\n",
        "                thickness=self.text_thickness)\n",
        "        return annotated_image"
      ],
      "metadata": {
        "id": "bJ_KZZ9UDeY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using all the classes for final output."
      ],
      "metadata": {
        "id": "L_BidfZHDdao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# settings\n",
        "# SOURCE_VIDEO_PATH = f\"{HOME}/clips/0a2d9b_0.mp4\"\n",
        "# TARGET_VIDEO_PATH = f\"{HOME}/final/0a2d9b_0.mp4\"\n",
        "\n",
        "SOURCE_VIDEO_PATH = f\"{HOME}/YOUR SOURCE VIDEO PATH\"\n",
        "\n",
        "TARGET_VIDEO_PATH = f\"{HOME}/YOUR TARGET VIDEO PATH\""
      ],
      "metadata": {
        "id": "n4TI2JEsDq4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice below how useful the get_average_color function is and how it splits players into either left or red through class id reassignment.\n"
      ],
      "metadata": {
        "id": "hBVX0HqsDo6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "COLORS = [\n",
        "    BALL_COLOR,\n",
        "    GOALKEEPER_COLOR,\n",
        "    PLAYER_COLOR,\n",
        "    REFEREE_COLOR,\n",
        "    LEFT_COLOR,\n",
        "    RIGHT_COLOR\n",
        "]\n",
        "# initiate video writer\n",
        "video_config = VideoConfig(\n",
        "    fps=30,\n",
        "    width=1920,\n",
        "    height=1080)\n",
        "video_writer = get_video_writer(\n",
        "    target_video_path=TARGET_VIDEO_PATH,\n",
        "    video_config=video_config)\n",
        "\n",
        "# get fresh video frame generator\n",
        "frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))\n",
        "\n",
        "# initiate annotators\n",
        "base_annotator = BaseAnnotator(\n",
        "    colors=COLORS,\n",
        "    thickness=THICKNESS)\n",
        "\n",
        "player_goalkeeper_text_annotator = TextAnnotator(\n",
        "    PLAYER_COLOR, text_color=Color(255, 255, 255), text_thickness=2)\n",
        "referee_text_annotator = TextAnnotator(\n",
        "    REFEREE_COLOR, text_color=Color(0, 0, 0), text_thickness=2)\n",
        "\n",
        "ball_marker_annotator = MarkerAnntator(\n",
        "    color=BALL_MARKER_FILL_COLOR)\n",
        "player_in_possession_marker_annotator = MarkerAnntator(\n",
        "    color=PLAYER_MARKER_FILL_COLOR)\n",
        "\n",
        "\n",
        "# initiate tracker\n",
        "byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "\n",
        "# loop over frames\n",
        "for frame in tqdm(frame_iterator):\n",
        "\n",
        "    # run detector\n",
        "    results = model(frame, size=1280)\n",
        "    detections = Detection.from_results(\n",
        "        pred=results.pred[0].cpu().numpy(),\n",
        "        names=model.names)\n",
        "\n",
        "    # filter detections by class\n",
        "    ball_detections = filter_detections_by_class(detections=detections, class_name=\"ball\")\n",
        "    referee_detections = filter_detections_by_class(detections=detections, class_name=\"referee\")\n",
        "    goalkeeper_detections = filter_detections_by_class(detections=detections, class_name=\"goalkeeper\")\n",
        "    player_detections = filter_detections_by_class(detections=detections, class_name=\"player\")\n",
        "\n",
        "    for x in list(player_detections):\n",
        "      x.player_attribute = get_average_color(frame, x.rect,102)\n",
        "    print(list([x.player_attribute for x in list(player_detections)]))\n",
        "\n",
        "    player_goalkeeper_detections = player_detections + goalkeeper_detections\n",
        "    tracked_detections = player_detections + goalkeeper_detections + referee_detections\n",
        "\n",
        "    # calculate player in possession\n",
        "    player_in_possession_detection = get_player_in_possession(\n",
        "        player_detections=player_goalkeeper_detections,\n",
        "        ball_detections=ball_detections,\n",
        "        proximity=PLAYER_IN_POSSESSION_PROXIMITY)\n",
        "\n",
        "    # track\n",
        "    tracks = byte_tracker.update(\n",
        "        output_results=detections2boxes(detections=tracked_detections),\n",
        "        img_info=frame.shape,\n",
        "        img_size=frame.shape\n",
        "    )\n",
        "    tracked_detections = match_detections_with_tracks(detections=tracked_detections, tracks=tracks)\n",
        "\n",
        "    tracked_referee_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"referee\")\n",
        "    tracked_goalkeeper_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"goalkeeper\")\n",
        "    tracked_player_detections = filter_detections_by_class(detections=tracked_detections, class_name=\"player\")\n",
        "\n",
        "    # annotate video frame\n",
        "    annotated_image = frame.copy()\n",
        "    annotated_image = base_annotator.annotate(\n",
        "        image=annotated_image,\n",
        "        detections=tracked_detections)\n",
        "\n",
        "    annotated_image = player_goalkeeper_text_annotator.annotate(\n",
        "        image=annotated_image,\n",
        "        detections=tracked_goalkeeper_detections + tracked_player_detections)\n",
        "    annotated_image = referee_text_annotator.annotate(\n",
        "        image=annotated_image,\n",
        "        detections=tracked_referee_detections)\n",
        "\n",
        "    annotated_image = ball_marker_annotator.annotate(\n",
        "        image=annotated_image,\n",
        "        detections=ball_detections)\n",
        "    annotated_image = player_marker_annotator.annotate(\n",
        "        image=annotated_image,\n",
        "        detections=[player_in_possession_detection] if player_in_possession_detection else [])\n",
        "    # cv2_imshow(annotated_image)\n",
        "\n",
        "    # save video frame\n",
        "    video_writer.write(annotated_image)\n",
        "    print(annotated_image.shape)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# close output video\n",
        "video_writer.release()"
      ],
      "metadata": {
        "id": "XbgXJzOMECnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js2iWTa5EDBV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}